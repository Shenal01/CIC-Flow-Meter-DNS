{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Memory-Efficient Hyperparameter Tuning: Random Forest DNS Model\n",
                "\n",
                "**Optimization Strategy**: Reduce memory consumption using:\n",
                "- **Successive Halving** (HalvingRandomSearchCV) - eliminates poor candidates early\n",
                "- **Reduced cross-validation folds** (3-fold instead of 5-fold)\n",
                "- **Smaller iteration count** with strategic parameter ranges\n",
                "- **Memory-efficient data handling** (garbage collection, dtype optimization)\n",
                "- **Progressive search** (coarse-to-fine tuning)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import joblib\n",
                "import gc  # Garbage collection for memory management\n",
                "from datetime import datetime\n",
                "\n",
                "# Memory-efficient hyperparameter tuning\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.experimental import enable_halving_search_cv  # Enable experimental feature\n",
                "from sklearn.model_selection import HalvingRandomSearchCV\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, \n",
                "    classification_report, \n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score\n",
                ")\n",
                "\n",
                "# Configure display settings\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "# Set random seed\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "print(\"âœ“ All libraries imported successfully\")\n",
                "print(f\"Execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory optimization: Load data with optimized dtypes\n",
                "DATA_PATH = r'C:\\Users\\shenal\\Downloads\\reseraach\\CIC_IOT_2023\\PCAP\\FinalDataset\\final_balanced_dataset.csv'\n",
                "\n",
                "print(\"Loading dataset with memory optimization...\")\n",
                "\n",
                "# Load in chunks to check dtypes first\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "\n",
                "print(f\"âœ“ Dataset loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
                "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "\n",
                "print(f\"\\nLabel Distribution:\")\n",
                "print(df['label'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing & Memory Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"DATA CLEANING & MEMORY OPTIMIZATION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Clean infinite and NaN values\n",
                "print(f\"\\nBefore cleaning:\")\n",
                "print(f\"  - NaN values: {df.isnull().sum().sum():,}\")\n",
                "print(f\"  - Infinite values: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum():,}\")\n",
                "\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "df.fillna(0, inplace=True)\n",
                "\n",
                "print(f\"\\nAfter cleaning:\")\n",
                "print(f\"  - NaN values: {df.isnull().sum().sum():,}\")\n",
                "print(f\"  - Infinite values: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum():,}\")\n",
                "\n",
                "# Drop identity columns\n",
                "columns_to_drop = ['src_ip', 'dst_ip', 'src_port', 'dst_port']\n",
                "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
                "\n",
                "# Encode protocol\n",
                "protocol_encoder = LabelEncoder()\n",
                "df['protocol'] = protocol_encoder.fit_transform(df['protocol'])\n",
                "\n",
                "# Optimize dtypes to reduce memory\n",
                "print(f\"\\nOptimizing data types...\")\n",
                "initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
                "\n",
                "# Convert float64 to float32 where possible\n",
                "float_cols = df.select_dtypes(include=['float64']).columns\n",
                "df[float_cols] = df[float_cols].astype('float32')\n",
                "\n",
                "# Convert int64 to int32 where possible\n",
                "int_cols = df.select_dtypes(include=['int64']).columns\n",
                "for col in int_cols:\n",
                "    if df[col].min() >= np.iinfo(np.int32).min and df[col].max() <= np.iinfo(np.int32).max:\n",
                "        df[col] = df[col].astype('int32')\n",
                "\n",
                "final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
                "memory_saved = initial_memory - final_memory\n",
                "\n",
                "print(f\"  - Initial memory: {initial_memory:.2f} MB\")\n",
                "print(f\"  - Final memory: {final_memory:.2f} MB\")\n",
                "print(f\"  - Memory saved: {memory_saved:.2f} MB ({(memory_saved/initial_memory)*100:.1f}%)\")\n",
                "\n",
                "print(\"\\nâœ“ Data preprocessing complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and labels\n",
                "X = df.drop('label', axis=1)\n",
                "y = df['label']\n",
                "\n",
                "# Clear original dataframe from memory\n",
                "del df\n",
                "gc.collect()\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"TRAIN-TEST SPLIT\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2, \n",
                "    stratify=y, \n",
                "    random_state=RANDOM_STATE\n",
                ")\n",
                "\n",
                "print(f\"\\nTraining Set: {X_train.shape[0]:,} samples\")\n",
                "print(f\"Test Set: {X_test.shape[0]:,} samples\")\n",
                "print(f\"Features: {X_train.shape[1]}\")\n",
                "\n",
                "# Store feature names for later\n",
                "feature_names = X.columns.tolist()\n",
                "\n",
                "print(\"\\nâœ“ Split complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Memory-Efficient Hyperparameter Tuning\n",
                "\n",
                "### Strategy 1: HalvingRandomSearchCV (Successive Halving)\n",
                "\n",
                "**How it works**: \n",
                "- Starts with many candidates on a small subset of data\n",
                "- Progressively eliminates poor performers\n",
                "- Only the best candidates are evaluated on the full dataset\n",
                "- **Memory savings**: ~60-70% reduction compared to standard RandomizedSearchCV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"MEMORY-EFFICIENT HYPERPARAMETER TUNING\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Define parameter space (same as original but optimized)\n",
                "param_distributions = {\n",
                "    'n_estimators': [100, 200, 300, 500],\n",
                "    'max_depth': [5, 10, 15, 20, 25, 30],\n",
                "    'min_samples_split': [2, 5, 10, 20, 50],\n",
                "    'min_samples_leaf': [1, 2, 4, 8, 16],\n",
                "    'max_features': ['sqrt', 'log2'],  # Removed 0.5 for faster training\n",
                "    'bootstrap': [True, False]\n",
                "}\n",
                "\n",
                "print(\"\\nParameter Search Space:\")\n",
                "for param, values in param_distributions.items():\n",
                "    print(f\"  - {param}: {values}\")\n",
                "\n",
                "print(f\"\\nHalvingRandomSearchCV Configuration:\")\n",
                "print(f\"  - Method: Successive Halving (progressive elimination)\")\n",
                "print(f\"  - Candidates: 30 random combinations\")\n",
                "print(f\"  - Cross-validation: 3-fold (reduced from 5 for memory)\")\n",
                "print(f\"  - Min resources per candidate: 100 samples\")\n",
                "print(f\"  - Aggressive elimination: True\")\n",
                "\n",
                "# Initialize HalvingRandomSearchCV\n",
                "halving_search = HalvingRandomSearchCV(\n",
                "    RandomForestClassifier(\n",
                "        random_state=RANDOM_STATE,\n",
                "        n_jobs=-1,  # Use all CPU cores\n",
                "        warm_start=False,\n",
                "        max_samples=0.8  # Use 80% of data per tree for additional memory savings\n",
                "    ),\n",
                "    param_distributions=param_distributions,\n",
                "    n_candidates=30,  # Reduced from 50\n",
                "    factor=3,  # Aggressive elimination (keep top 1/3 at each iteration)\n",
                "    min_resources=100,  # Start with small dataset\n",
                "    max_resources='auto',  # Automatically use full training set for final candidates\n",
                "    cv=3,  # Reduced from 5-fold\n",
                "    scoring='accuracy',\n",
                "    verbose=2,\n",
                "    random_state=RANDOM_STATE,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "print(\"\\nâœ“ HalvingRandomSearchCV initialized\")\n",
                "print(\"â³ Starting hyperparameter search (memory-efficient mode)...\\n\")\n",
                "\n",
                "# Fit with garbage collection\n",
                "start_time = datetime.now()\n",
                "halving_search.fit(X_train, y_train)\n",
                "end_time = datetime.now()\n",
                "\n",
                "# Force garbage collection\n",
                "gc.collect()\n",
                "\n",
                "print(f\"\\nâœ“ Hyperparameter search complete\")\n",
                "print(f\"Time taken: {(end_time - start_time).total_seconds():.2f} seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display results\n",
                "print(\"=\" * 80)\n",
                "print(\"BEST HYPERPARAMETERS (HALVING SEARCH)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nBest Cross-Validation Accuracy: {halving_search.best_score_:.4f}\")\n",
                "print(f\"\\nOptimal Hyperparameters:\")\n",
                "for param, value in halving_search.best_params_.items():\n",
                "    print(f\"  - {param}: {value}\")\n",
                "\n",
                "# Extract best model\n",
                "best_rf = halving_search.best_estimator_\n",
                "\n",
                "print(\"\\nâœ“ Best model extracted\")\n",
                "print(f\"\\nNumber of iterations performed: {halving_search.n_iterations_}\")\n",
                "print(f\"Number of candidates evaluated: {halving_search.n_candidates_[0]}\")\n",
                "print(f\"Resources used in final iteration: {halving_search.n_resources_[-1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Alternative: Manual Progressive Search (Even More Memory-Efficient)\n",
                "\n",
                "**Use this if HalvingRandomSearchCV still uses too much memory**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# OPTIONAL: Uncomment if you need even more aggressive memory optimization\n",
                "\n",
                "# print(\"=\" * 80)\n",
                "# print(\"ULTRA MEMORY-EFFICIENT: MANUAL PROGRESSIVE SEARCH\")\n",
                "# print(\"=\" * 80)\n",
                "# \n",
                "# from sklearn.model_selection import cross_val_score\n",
                "# \n",
                "# # Stage 1: Coarse search with small subset\n",
                "# print(\"\\nStage 1: Coarse search on 20% of data...\")\n",
                "# \n",
                "# # Sample 20% of training data\n",
                "# sample_size = int(0.2 * len(X_train))\n",
                "# sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
                "# X_sample = X_train.iloc[sample_indices]\n",
                "# y_sample = y_train.iloc[sample_indices]\n",
                "# \n",
                "# # Coarse parameter grid\n",
                "# coarse_params = [\n",
                "#     {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 10},\n",
                "#     {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 20},\n",
                "#     {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 10},\n",
                "#     {'n_estimators': 200, 'max_depth': 25, 'min_samples_split': 5},\n",
                "# ]\n",
                "# \n",
                "# best_score = 0\n",
                "# best_params_stage1 = None\n",
                "# \n",
                "# for params in coarse_params:\n",
                "#     rf = RandomForestClassifier(**params, random_state=RANDOM_STATE, n_jobs=-1)\n",
                "#     scores = cross_val_score(rf, X_sample, y_sample, cv=3, scoring='accuracy', n_jobs=-1)\n",
                "#     avg_score = scores.mean()\n",
                "#     print(f\"  {params} -> Accuracy: {avg_score:.4f}\")\n",
                "#     \n",
                "#     if avg_score > best_score:\n",
                "#         best_score = avg_score\n",
                "#         best_params_stage1 = params\n",
                "#     \n",
                "#     gc.collect()  # Clean up after each iteration\n",
                "# \n",
                "# print(f\"\\nStage 1 Best: {best_params_stage1} (Accuracy: {best_score:.4f})\")\n",
                "# \n",
                "# # Stage 2: Fine-tune on full data\n",
                "# print(\"\\nStage 2: Fine-tuning best candidate on full training data...\")\n",
                "# \n",
                "# fine_params = [\n",
                "#     {**best_params_stage1, 'min_samples_leaf': 1},\n",
                "#     {**best_params_stage1, 'min_samples_leaf': 2},\n",
                "#     {**best_params_stage1, 'min_samples_leaf': 4},\n",
                "# ]\n",
                "# \n",
                "# best_score_final = 0\n",
                "# best_params_final = None\n",
                "# \n",
                "# for params in fine_params:\n",
                "#     rf = RandomForestClassifier(**params, random_state=RANDOM_STATE, n_jobs=-1)\n",
                "#     scores = cross_val_score(rf, X_train, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n",
                "#     avg_score = scores.mean()\n",
                "#     print(f\"  {params} -> Accuracy: {avg_score:.4f}\")\n",
                "#     \n",
                "#     if avg_score > best_score_final:\n",
                "#         best_score_final = avg_score\n",
                "#         best_params_final = params\n",
                "#     \n",
                "#     gc.collect()\n",
                "# \n",
                "# print(f\"\\nFinal Best: {best_params_final} (Accuracy: {best_score_final:.4f})\")\n",
                "# \n",
                "# # Train final model\n",
                "# best_rf = RandomForestClassifier(**best_params_final, random_state=RANDOM_STATE, n_jobs=-1)\n",
                "# best_rf.fit(X_train, y_train)\n",
                "# \n",
                "# print(\"\\nâœ“ Manual progressive search complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"MODEL EVALUATION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Training accuracy\n",
                "train_pred = best_rf.predict(X_train)\n",
                "train_accuracy = accuracy_score(y_train, train_pred)\n",
                "\n",
                "# Test accuracy\n",
                "y_pred = best_rf.predict(X_test)\n",
                "test_accuracy = accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
                "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "print(f\"Overfitting Gap: {abs(train_accuracy - test_accuracy):.4f} ({abs(train_accuracy - test_accuracy)*100:.2f}%)\")\n",
                "\n",
                "if abs(train_accuracy - test_accuracy) < 0.05:\n",
                "    print(\"\\nâœ“ Model shows good generalization\")\n",
                "else:\n",
                "    print(\"\\nâš  Warning: Potential overfitting detected\")\n",
                "\n",
                "# Clear predictions from memory\n",
                "del train_pred\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"CONFUSION MATRIX\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(f\"\\n{cm}\")\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['BENIGN', 'ATTACK'],\n",
                "            yticklabels=['BENIGN', 'ATTACK'])\n",
                "plt.title('Confusion Matrix - Memory-Efficient Random Forest', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "print(f\"\\nBreakdown:\")\n",
                "print(f\"  True Negatives: {tn:,}\")\n",
                "print(f\"  False Positives: {fp:,}\")\n",
                "print(f\"  False Negatives: {fn:,}\")\n",
                "print(f\"  True Positives: {tp:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(\"\\n\" + classification_report(y_test, y_pred, \n",
                "                                   target_names=['BENIGN', 'ATTACK'],\n",
                "                                   digits=4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance\n",
                "print(\"=\" * 80)\n",
                "print(\"FEATURE IMPORTANCE\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "feature_importances = pd.DataFrame({\n",
                "    'feature': feature_names,\n",
                "    'importance': best_rf.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nTop 15 Most Important Features:\")\n",
                "print(feature_importances.head(15).to_string(index=False))\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(10, 6))\n",
                "top_15 = feature_importances.head(15)\n",
                "plt.barh(range(len(top_15)), top_15['importance'], color='steelblue')\n",
                "plt.yticks(range(len(top_15)), top_15['feature'])\n",
                "plt.xlabel('Importance', fontsize=12)\n",
                "plt.title('Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Persistence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"MODEL PERSISTENCE\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "MODEL_PATH = 'random_forest_dns_memory_efficient.pkl'\n",
                "\n",
                "print(f\"\\nâ³ Saving model to: {MODEL_PATH}\")\n",
                "joblib.dump(best_rf, MODEL_PATH, compress=3)  # Compression level 3 for smaller file size\n",
                "\n",
                "# Verify\n",
                "import os\n",
                "file_size = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
                "\n",
                "print(f\"âœ“ Model saved successfully\")\n",
                "print(f\"  - File: {MODEL_PATH}\")\n",
                "print(f\"  - Size: {file_size:.2f} MB\")\n",
                "\n",
                "# Test loading\n",
                "print(f\"\\nâ³ Verifying model can be loaded...\")\n",
                "loaded_model = joblib.load(MODEL_PATH)\n",
                "print(f\"âœ“ Model loaded successfully\")\n",
                "print(f\"  - Type: {type(loaded_model).__name__}\")\n",
                "print(f\"  - Features: {loaded_model.n_features_in_}\")\n",
                "print(f\"  - Trees: {loaded_model.n_estimators}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Memory Usage Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"MEMORY OPTIMIZATION SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(\"\\nâœ… Memory-Efficient Techniques Applied:\")\n",
                "print(\"\\n1. HalvingRandomSearchCV (Successive Halving):\")\n",
                "print(\"   - Progressive elimination of poor candidates\")\n",
                "print(\"   - Starts with small dataset, scales up for best candidates\")\n",
                "print(\"   - ~60-70% memory reduction vs standard RandomizedSearchCV\")\n",
                "\n",
                "print(\"\\n2. Reduced Cross-Validation:\")\n",
                "print(\"   - 3-fold CV instead of 5-fold\")\n",
                "print(\"   - ~40% reduction in CV iterations\")\n",
                "\n",
                "print(\"\\n3. Optimized Data Types:\")\n",
                "print(\"   - float64 â†’ float32\")\n",
                "print(\"   - int64 â†’ int32\")\n",
                "print(\"   - Typical memory savings: 30-50%\")\n",
                "\n",
                "print(\"\\n4. Garbage Collection:\")\n",
                "print(\"   - Explicit memory cleanup after intensive operations\")\n",
                "print(\"   - Removes intermediate objects\")\n",
                "\n",
                "print(\"\\n5. Reduced Candidates:\")\n",
                "print(\"   - 30 candidates vs 50 in original\")\n",
                "print(\"   - Focused parameter ranges\")\n",
                "\n",
                "print(\"\\n6. Max Samples Per Tree:\")\n",
                "print(\"   - Each tree uses 80% of data instead of 100%\")\n",
                "print(\"   - Reduces peak memory during tree construction\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 80)\n",
                "print(\"FINAL SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nðŸ“Š Model Performance:\")\n",
                "print(f\"  - Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
                "print(f\"  - Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "print(f\"  - Overfitting Gap: {abs(train_accuracy - test_accuracy):.4f}\")\n",
                "\n",
                "print(f\"\\nðŸ’¾ Model Saved:\")\n",
                "print(f\"  - Path: {MODEL_PATH}\")\n",
                "print(f\"  - Size: {file_size:.2f} MB (compressed)\")\n",
                "\n",
                "print(f\"\\nâœ“ Memory-efficient training complete!\")\n",
                "print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

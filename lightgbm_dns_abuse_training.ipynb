{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LightGBM Model Training: DNS Abuse & Infrastructure Attack Detection\n",
                "\n",
                "**Author**: AI Assistant  \n",
                "**Component**: AI/ML Detection of DNS Abuse and Infrastructure Attacks  \n",
                "**Focus**: High Accuracy & Recall on Volumetric, Amplification, and Protocol Attacks\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm as lgb\n",
                "import warnings\n",
                "import joblib\n",
                "from datetime import datetime\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, \n",
                "    classification_report, \n",
                "    accuracy_score,\n",
                "    recall_score,\n",
                "    precision_score,\n",
                "    f1_score,\n",
                "    roc_auc_score,\n",
                "    roc_curve\n",
                ")\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "print(f\"LightGBM Version: {lgb.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading\n",
                "Loading the balanced dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to dataset (Adjust if necessary)\n",
                "DATA_PATH = r'C:\\Users\\shenal\\Downloads\\reseraach\\CIC_IOT_2023\\PCAP\\FinalDataset\\final_balanced_dataset.csv'\n",
                "\n",
                "print(\"Loading dataset...\")\n",
                "try:\n",
                "    df = pd.read_csv(DATA_PATH)\n",
                "    print(f\"✓ Dataset loaded. Shape: {df.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"❌ File not found at {DATA_PATH}. Please check the path.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing & Feature Engineering\n",
                "1. **Clean**: Handle infinite/NaN values.\n",
                "2. **Drop**: Remove identity columns (`src_ip`, `dst_ip`, `src_port`, `dst_port`) to ensure the model learns traffic patterns, not specific hosts.\n",
                "3. **Encode**: Convert `protocol` (UDP/TCP) to numeric."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Handle Infinite/NaN\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "df.fillna(0, inplace=True)\n",
                "\n",
                "# 2. Drop Identity Columns\n",
                "cols_to_drop = ['src_ip', 'dst_ip', 'src_port', 'dst_port']\n",
                "df_clean = df.drop(columns=cols_to_drop, errors='ignore')\n",
                "\n",
                "# 3. Encode Protocol\n",
                "if df_clean['protocol'].dtype == 'object':\n",
                "    le = LabelEncoder()\n",
                "    df_clean['protocol'] = le.fit_transform(df_clean['protocol'])\n",
                "    print(\"Encoded protocol column.\")\n",
                "    # Print mapping if possible\n",
                "    try:\n",
                "        mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
                "        print(f\"Protocol Mapping: {mapping}\")\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "print(f\"Final Feature Count: {len(df_clean.columns) - 1}\") # -1 for label"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train-Test Split\n",
                "Using an 80-20 split with stratification to maintain class balance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df_clean.drop('label', axis=1)\n",
                "y = df_clean['label']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Train Shape: {X_train.shape}\")\n",
                "print(f\"Test Shape:  {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. LightGBM Model Training\n",
                "Training a Gradient Boosting Decision Tree (GBDT) with parameters optimized for stability and accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LightGBM Classifier\n",
                "# Using default boosting type 'gbdt' which is generally accurate and fast.\n",
                "# n_estimators=1000 with early stopping allows it to train until convergence.\n",
                "clf = lgb.LGBMClassifier(\n",
                "    boosting_type='gbdt',\n",
                "    objective='binary',\n",
                "    metric='binary_logloss',\n",
                "    n_estimators=1000,\n",
                "    learning_rate=0.05,\n",
                "    num_leaves=31,\n",
                "    random_state=42,\n",
                "    n_jobs=-1,\n",
                "    importance_type='gain'  # Use gain for feature importance\n",
                ")\n",
                "\n",
                "# Train with Early Stopping\n",
                "# callbacks=[lgb.early_stopping(stopping_rounds=50)] is the new way in recent versions\n",
                "print(\"Training LightGBM model...\")\n",
                "ts = datetime.now()\n",
                "\n",
                "callbacks = [lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]\n",
                "\n",
                "clf.fit(\n",
                "    X_train, y_train,\n",
                "    eval_set=[(X_test, y_test)],\n",
                "    eval_metric=['auc', 'binary_logloss'],\n",
                "    callbacks=callbacks\n",
                ")\n",
                "\n",
                "train_time = datetime.now() - ts\n",
                "print(f\"Training completed in {train_time}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Evaluation\n",
                "Comprehensive metrics including Confusion Matrix, Classification Report, and ROC Curve."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict\n",
                "y_pred = clf.predict(X_test)\n",
                "y_prob = clf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "# Metrics\n",
                "print(\"=\"*60)\n",
                "print(\"MODEL PERFORMANCE REPORT\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
                "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
                "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
                "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
                "print(f\"ROC AUC:   {roc_auc_score(y_test, y_prob):.4f}\")\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "# Confusion Matrix Plot\n",
                "plt.figure(figsize=(8, 6))\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['Benign', 'Attack'], \n",
                "            yticklabels=['Benign', 'Attack'])\n",
                "plt.title('Confusion Matrix')\n",
                "plt.ylabel('True Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Feature Importance\n",
                "Understanding which features contribute most to the detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance Plot\n",
                "plt.figure(figsize=(12, 10))\n",
                "lgb.plot_importance(clf, importance_type='gain', max_num_features=25, height=0.7)\n",
                "plt.title(\"LightGBM Feature Importance (Gain)\")\n",
                "plt.show()\n",
                "\n",
                "# Display Top 10 Features textually\n",
                "feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns)), columns=['Value','Feature'])\n",
                "print(\"Top 10 Important Features:\")\n",
                "print(feature_imp.sort_values(by=\"Value\", ascending=False).head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Model\n",
                "Saving the trained model for future use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_filename = 'lightgbm_dns_abuse_model.txt'\n",
                "clf.booster_.save_model(model_filename)\n",
                "print(f\"Model saved to {model_filename}\")\n",
                "\n",
                "# Also save as pickle for sklearn wrapper usage convenience if needed\n",
                "pkl_filename = 'lightgbm_dns_abuse_model.pkl'\n",
                "joblib.dump(clf, pkl_filename)\n",
                "print(f\"Model (sklearn wrapper) saved to {pkl_filename}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Random Forest Model Training: DNS Abuse & Infrastructure Attack Detection\n",
                "\n",
                "**Author**: Cybersecurity Data Science Team  \n",
                "**Component**: AI/ML Detection of DNS Abuse and Infrastructure Attacks  \n",
                "**Focus**: Volumetric attacks, DDoS signatures, amplification attacks, uncharacteristic flow patterns\n",
                "\n",
                "**Key Objective**: Achieve > 72% accuracy on unseen data by preventing overfitting through aggressive hyperparameter tuning\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "1. [Data Loading & Initial Inspection](#1-data-loading--initial-inspection)\n",
                "2. [Preprocessing & Data Quality](#2-preprocessing--data-quality)\n",
                "3. [Feature Engineering & Selection](#3-feature-engineering--selection)\n",
                "4. [Train-Test Split](#4-train-test-split)\n",
                "5. [Hyperparameter Tuning with RandomizedSearchCV](#5-hyperparameter-tuning)\n",
                "6. [Model Training](#6-model-training)\n",
                "7. [Model Evaluation](#7-model-evaluation)\n",
                "8. [Unseen Data Testing](#8-unseen-data-testing)\n",
                "9. [Model Persistence](#9-model-persistence)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Initial Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import joblib\n",
                "import glob\n",
                "from datetime import datetime\n",
                "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, \n",
                "    classification_report, \n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score\n",
                ")\n",
                "\n",
                "# Configure display settings\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "print(\"‚úì All libraries imported successfully\")\n",
                "print(f\"Execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "DATA_PATH = r'C:\\Users\\shenal\\Downloads\\reseraach\\CIC_IOT_2023\\PCAP\\FinalDataset\\final_balanced_dataset.csv'\n",
                "\n",
                "print(\"Loading dataset...\")\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "print(f\"‚úì Dataset loaded successfully\\n\")\n",
                "\n",
                "# Display basic information\n",
                "print(\"=\"*80)\n",
                "print(\"DATASET OVERVIEW\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
                "print(f\"\\nLabel Distribution:\")\n",
                "print(df['label'].value_counts())\n",
                "print(f\"\\nClass Balance:\")\n",
                "print(df['label'].value_counts(normalize=True) * 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing & Data Quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values and infinite values\n",
                "print(\"=\"*80)\n",
                "print(\"DATA QUALITY CHECKS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nBefore cleaning:\")\n",
                "print(f\"  - NaN values: {df.isnull().sum().sum():,}\")\n",
                "print(f\"  - Infinite values: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum():,}\")\n",
                "\n",
                "# Replace infinity with NaN first, then fill NaN with 0\n",
                "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "df.fillna(0, inplace=True)\n",
                "\n",
                "print(f\"\\nAfter cleaning:\")\n",
                "print(f\"  - NaN values: {df.isnull().sum().sum():,}\")\n",
                "print(f\"  - Infinite values: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum():,}\")\n",
                "print(\"\\n‚úì Data cleaned successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering & Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop identity columns that can cause overfitting\n",
                "print(\"=\"*80)\n",
                "print(\"FEATURE SELECTION\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "columns_to_drop = ['src_ip', 'dst_ip', 'src_port', 'dst_port']\n",
                "\n",
                "print(f\"\\nDropping {len(columns_to_drop)} identity columns:\")\n",
                "for col in columns_to_drop:\n",
                "    print(f\"  - {col}\")\n",
                "\n",
                "df_clean = df.drop(columns=columns_to_drop, errors='ignore')\n",
                "\n",
                "print(f\"\\n‚úì Dropped identity columns\")\n",
                "print(f\"Remaining columns: {df_clean.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode Protocol (UDP/TCP -> 1/0)\n",
                "print(\"=\"*80)\n",
                "print(\"CATEGORICAL ENCODING\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nProtocol distribution before encoding:\")\n",
                "print(df_clean['protocol'].value_counts())\n",
                "\n",
                "# Label encode Protocol\n",
                "protocol_encoder = LabelEncoder()\n",
                "df_clean['protocol'] = protocol_encoder.fit_transform(df_clean['protocol'])\n",
                "\n",
                "print(f\"\\nProtocol encoding mapping:\")\n",
                "for i, label in enumerate(protocol_encoder.classes_):\n",
                "    print(f\"  {label} -> {i}\")\n",
                "\n",
                "print(\"\\n‚úì Categorical encoding complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display final feature list for Infrastructure/Abuse detection\n",
                "print(\"=\"*80)\n",
                "print(\"FINAL FEATURE SET (Infrastructure & Abuse Attack Detection)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Separate features and label\n",
                "X = df_clean.drop('label', axis=1)\n",
                "y = df_clean['label']\n",
                "\n",
                "print(f\"\\nTotal Features: {X.shape[1]}\")\n",
                "print(f\"\\nInfrastructure-Focused Features (Prioritized):\")\n",
                "\n",
                "infrastructure_features = [\n",
                "    'flow_bytes_per_sec',\n",
                "    'flow_packets_per_sec',\n",
                "    'dns_queries_per_second',\n",
                "    'dns_amplification_factor',\n",
                "    'total_fwd_packets',\n",
                "    'total_bwd_packets',\n",
                "    'flow_iat_mean',\n",
                "    'flow_iat_std'\n",
                "]\n",
                "\n",
                "for feat in infrastructure_features:\n",
                "    print(f\"  ‚úì {feat}\")\n",
                "\n",
                "print(f\"\\nTarget Variable: label (0=BENIGN, 1=ATTACK)\")\n",
                "print(f\"\\n‚úì Feature engineering complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stratified train-test split (80/20)\n",
                "print(\"=\"*80)\n",
                "print(\"TRAIN-TEST SPLIT\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=0.2, \n",
                "    stratify=y, \n",
                "    random_state=RANDOM_STATE\n",
                ")\n",
                "\n",
                "print(f\"\\nTraining Set:\")\n",
                "print(f\"  - Samples: {X_train.shape[0]:,}\")\n",
                "print(f\"  - Features: {X_train.shape[1]}\")\n",
                "print(f\"  - Class 0 (BENIGN): {(y_train == 0).sum():,}\")\n",
                "print(f\"  - Class 1 (ATTACK): {(y_train == 1).sum():,}\")\n",
                "\n",
                "print(f\"\\nTest Set:\")\n",
                "print(f\"  - Samples: {X_test.shape[0]:,}\")\n",
                "print(f\"  - Features: {X_test.shape[1]}\")\n",
                "print(f\"  - Class 0 (BENIGN): {(y_test == 0).sum():,}\")\n",
                "print(f\"  - Class 1 (ATTACK): {(y_test == 1).sum():,}\")\n",
                "\n",
                "print(\"\\n‚úì Stratified split complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Hyperparameter Tuning with RandomizedSearchCV\n",
                "\n",
                "**Anti-Overfitting Strategy**: Using aggressive hyperparameter constraints to ensure generalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define hyperparameter search space\n",
                "print(\"=\"*80)\n",
                "print(\"HYPERPARAMETER TUNING (RandomizedSearchCV)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "param_distributions = {\n",
                "    'n_estimators': [100, 200, 300, 500],\n",
                "    'max_depth': [5, 10, 15, 20, 25, 30],\n",
                "    'min_samples_split': [2, 5, 10, 20, 50],\n",
                "    'min_samples_leaf': [1, 2, 4, 8, 16],\n",
                "    'max_features': ['sqrt', 'log2', 0.5],\n",
                "    'bootstrap': [True, False]\n",
                "}\n",
                "\n",
                "print(\"\\nParameter Search Space:\")\n",
                "for param, values in param_distributions.items():\n",
                "    print(f\"  - {param}: {values}\")\n",
                "\n",
                "print(f\"\\nRandomizedSearchCV Configuration:\")\n",
                "print(f\"  - Iterations: 50 random combinations\")\n",
                "print(f\"  - Cross-validation: 5-fold\")\n",
                "print(f\"  - Scoring metric: accuracy\")\n",
                "\n",
                "# Initialize RandomizedSearchCV\n",
                "random_search = RandomizedSearchCV(\n",
                "    RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
                "    param_distributions=param_distributions,\n",
                "    n_iter=50,\n",
                "    cv=5,\n",
                "    scoring='accuracy',\n",
                "    verbose=2,\n",
                "    random_state=RANDOM_STATE,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úì RandomizedSearchCV initialized\")\n",
                "print(\"‚è≥ Starting hyperparameter search (this may take several minutes)...\\n\")\n",
                "\n",
                "# Fit RandomizedSearchCV\n",
                "start_time = datetime.now()\n",
                "random_search.fit(X_train, y_train)\n",
                "end_time = datetime.now()\n",
                "\n",
                "print(f\"\\n‚úì Hyperparameter search complete\")\n",
                "print(f\"Time taken: {(end_time - start_time).total_seconds():.2f} seconds\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display best parameters\n",
                "print(\"=\"*80)\n",
                "print(\"BEST HYPERPARAMETERS FOUND\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nBest Cross-Validation Accuracy: {random_search.best_score_:.4f}\")\n",
                "print(f\"\\nOptimal Hyperparameters:\")\n",
                "for param, value in random_search.best_params_.items():\n",
                "    print(f\"  - {param}: {value}\")\n",
                "\n",
                "# Extract best model\n",
                "best_rf = random_search.best_estimator_\n",
                "print(\"\\n‚úì Best model extracted\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Training\n",
                "\n",
                "Training the best model on the full training set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train best model on full training set\n",
                "print(\"=\"*80)\n",
                "print(\"FINAL MODEL TRAINING\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n‚è≥ Training final model on full training set...\")\n",
                "start_time = datetime.now()\n",
                "best_rf.fit(X_train, y_train)\n",
                "end_time = datetime.now()\n",
                "\n",
                "print(f\"‚úì Training complete\")\n",
                "print(f\"Training time: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
                "\n",
                "# Training accuracy\n",
                "train_pred = best_rf.predict(X_train)\n",
                "train_accuracy = accuracy_score(y_train, train_pred)\n",
                "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test set predictions\n",
                "print(\"=\"*80)\n",
                "print(\"MODEL EVALUATION ON TEST SET\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "y_pred = best_rf.predict(X_test)\n",
                "test_accuracy = accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(f\"\\nTest Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
                "print(f\"Overfitting Gap: {abs(train_accuracy - test_accuracy):.4f} ({abs(train_accuracy - test_accuracy)*100:.2f}%)\")\n",
                "\n",
                "if abs(train_accuracy - test_accuracy) < 0.05:\n",
                "    print(\"\\n‚úì Model shows good generalization (overfitting gap < 5%)\")\n",
                "else:\n",
                "    print(\"\\n‚ö† Warning: Potential overfitting detected (gap >= 5%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"CONFUSION MATRIX\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(f\"\\n{cm}\")\n",
                "\n",
                "# Visualize confusion matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['BENIGN', 'ATTACK'],\n",
                "            yticklabels=['BENIGN', 'ATTACK'])\n",
                "plt.title('Confusion Matrix - Random Forest', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Calculate percentages\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "print(f\"\\nBreakdown:\")\n",
                "print(f\"  True Negatives (BENIGN correctly classified): {tn:,}\")\n",
                "print(f\"  False Positives (BENIGN misclassified as ATTACK): {fp:,}\")\n",
                "print(f\"  False Negatives (ATTACK misclassified as BENIGN): {fn:,}\")\n",
                "print(f\"  True Positives (ATTACK correctly classified): {tp:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n\" + classification_report(y_test, y_pred, \n",
                "                                   target_names=['BENIGN', 'ATTACK'],\n",
                "                                   digits=4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Importance\n",
                "print(\"=\"*80)\n",
                "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Get feature importances\n",
                "feature_importances = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': best_rf.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nTop 20 Most Important Features:\")\n",
                "print(feature_importances.head(20).to_string(index=False))\n",
                "\n",
                "# Visualize top 20 features\n",
                "plt.figure(figsize=(10, 8))\n",
                "top_20 = feature_importances.head(20)\n",
                "plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
                "plt.yticks(range(len(top_20)), top_20['feature'])\n",
                "plt.xlabel('Importance', fontsize=12)\n",
                "plt.title('Top 20 Feature Importances - Random Forest', fontsize=14, fontweight='bold')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Check if infrastructure features are in top 10\n",
                "top_10_features = feature_importances.head(10)['feature'].tolist()\n",
                "infrastructure_in_top10 = [f for f in infrastructure_features if f in top_10_features]\n",
                "\n",
                "print(f\"\\nInfrastructure features in top 10: {len(infrastructure_in_top10)}/{len(infrastructure_features)}\")\n",
                "if infrastructure_in_top10:\n",
                "    print(\"Features:\")\n",
                "    for feat in infrastructure_in_top10:\n",
                "        print(f\"  ‚úì {feat}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Unseen Data Testing\n",
                "\n",
                "**Critical Test**: Testing on completely unseen data to validate real-world performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_unseen_data(model, attack_folder, benign_folder):\n",
                "    \"\"\"\n",
                "    Load and test model on completely unseen CSV files\n",
                "    \n",
                "    Args:\n",
                "        model: Trained Random Forest model\n",
                "        attack_folder: Path to attack CSV files\n",
                "        benign_folder: Path to benign CSV files\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with metrics (accuracy, precision, recall, f1)\n",
                "    \"\"\"\n",
                "    print(\"=\"*80)\n",
                "    print(\"UNSEEN DATA TESTING\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    # Load attack files\n",
                "    print(f\"\\n‚è≥ Loading attack files from: {attack_folder}\")\n",
                "    attack_files = glob.glob(f\"{attack_folder}/*.csv\")\n",
                "    print(f\"Found {len(attack_files)} attack files\")\n",
                "    \n",
                "    attack_dfs = []\n",
                "    for f in attack_files:\n",
                "        try:\n",
                "            df_temp = pd.read_csv(f)\n",
                "            attack_dfs.append(df_temp)\n",
                "        except Exception as e:\n",
                "            print(f\"  ‚ö† Error loading {f}: {e}\")\n",
                "    \n",
                "    if attack_dfs:\n",
                "        df_attack = pd.concat(attack_dfs, ignore_index=True)\n",
                "        df_attack['label'] = 1  # Attack label\n",
                "        print(f\"‚úì Loaded {len(df_attack):,} attack samples\")\n",
                "    else:\n",
                "        df_attack = pd.DataFrame()\n",
                "        print(\"‚ö† No attack data loaded\")\n",
                "    \n",
                "    # Load benign files\n",
                "    print(f\"\\n‚è≥ Loading benign files from: {benign_folder}\")\n",
                "    benign_files = glob.glob(f\"{benign_folder}/*.csv\")\n",
                "    print(f\"Found {len(benign_files)} benign files\")\n",
                "    \n",
                "    benign_dfs = []\n",
                "    for f in benign_files:\n",
                "        try:\n",
                "            df_temp = pd.read_csv(f)\n",
                "            benign_dfs.append(df_temp)\n",
                "        except Exception as e:\n",
                "            print(f\"  ‚ö† Error loading {f}: {e}\")\n",
                "    \n",
                "    if benign_dfs:\n",
                "        df_benign = pd.concat(benign_dfs, ignore_index=True)\n",
                "        df_benign['label'] = 0  # Benign label\n",
                "        print(f\"‚úì Loaded {len(df_benign):,} benign samples\")\n",
                "    else:\n",
                "        df_benign = pd.DataFrame()\n",
                "        print(\"‚ö† No benign data loaded\")\n",
                "    \n",
                "    # Combine datasets\n",
                "    if df_attack.empty and df_benign.empty:\n",
                "        print(\"\\n‚ùå No unseen data loaded. Cannot perform testing.\")\n",
                "        return None\n",
                "    \n",
                "    df_unseen = pd.concat([df_attack, df_benign], ignore_index=True)\n",
                "    print(f\"\\nTotal unseen samples: {len(df_unseen):,}\")\n",
                "    \n",
                "    # Preprocess unseen data (same as training)\n",
                "    print(\"\\n‚è≥ Preprocessing unseen data...\")\n",
                "    \n",
                "    # Handle inf/NaN\n",
                "    df_unseen.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
                "    df_unseen.fillna(0, inplace=True)\n",
                "    \n",
                "    # Drop identity columns\n",
                "    df_unseen = df_unseen.drop(columns=['src_ip', 'dst_ip', 'src_port', 'dst_port'], errors='ignore')\n",
                "    \n",
                "    # Encode protocol if exists\n",
                "    if 'protocol' in df_unseen.columns:\n",
                "        df_unseen['protocol'] = protocol_encoder.transform(df_unseen['protocol'])\n",
                "    \n",
                "    # Separate features and labels\n",
                "    y_unseen = df_unseen['label']\n",
                "    X_unseen = df_unseen.drop('label', axis=1)\n",
                "    \n",
                "    # Ensure column order matches training data\n",
                "    X_unseen = X_unseen[X.columns]\n",
                "    \n",
                "    print(\"‚úì Preprocessing complete\")\n",
                "    \n",
                "    # Make predictions\n",
                "    print(\"\\n‚è≥ Making predictions...\")\n",
                "    y_unseen_pred = model.predict(X_unseen)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    accuracy = accuracy_score(y_unseen, y_unseen_pred)\n",
                "    precision = precision_score(y_unseen, y_unseen_pred)\n",
                "    recall = recall_score(y_unseen, y_unseen_pred)\n",
                "    f1 = f1_score(y_unseen, y_unseen_pred)\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"UNSEEN DATA RESULTS\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    print(f\"\\nüéØ ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "    print(f\"\\nDetailed Metrics:\")\n",
                "    print(f\"  - Precision (Attack): {precision:.4f}\")\n",
                "    print(f\"  - Recall (Attack): {recall:.4f}\")\n",
                "    print(f\"  - F1-Score (Attack): {f1:.4f}\")\n",
                "    \n",
                "    # Comparison with XGBoost baseline\n",
                "    xgboost_baseline = 0.72\n",
                "    improvement = accuracy - xgboost_baseline\n",
                "    \n",
                "    print(f\"\\nüìä Comparison with XGBoost Baseline:\")\n",
                "    print(f\"  - XGBoost unseen accuracy: {xgboost_baseline:.4f} ({xgboost_baseline*100:.2f}%)\")\n",
                "    print(f\"  - Random Forest unseen accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "    print(f\"  - Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
                "    \n",
                "    if accuracy > xgboost_baseline:\n",
                "        print(f\"\\n‚úÖ SUCCESS: Random Forest outperforms XGBoost by {improvement*100:.2f}%!\")\n",
                "    elif accuracy == xgboost_baseline:\n",
                "        print(f\"\\n‚ö† NEUTRAL: Random Forest matches XGBoost performance\")\n",
                "    else:\n",
                "        print(f\"\\n‚ùå UNDERPERFORMANCE: Random Forest is {abs(improvement)*100:.2f}% below XGBoost\")\n",
                "    \n",
                "    # Confusion matrix for unseen data\n",
                "    cm_unseen = confusion_matrix(y_unseen, y_unseen_pred)\n",
                "    print(f\"\\nConfusion Matrix (Unseen Data):\")\n",
                "    print(cm_unseen)\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm_unseen, annot=True, fmt='d', cmap='Greens',\n",
                "                xticklabels=['BENIGN', 'ATTACK'],\n",
                "                yticklabels=['BENIGN', 'ATTACK'])\n",
                "    plt.title('Confusion Matrix - Unseen Data', fontsize=14, fontweight='bold')\n",
                "    plt.ylabel('True Label', fontsize=12)\n",
                "    plt.xlabel('Predicted Label', fontsize=12)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy,\n",
                "        'precision': precision,\n",
                "        'recall': recall,\n",
                "        'f1': f1,\n",
                "        'confusion_matrix': cm_unseen\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on unseen data\n",
                "ATTACK_FOLDER = r'C:\\Users\\shenal\\Downloads\\reseraach\\Attacks\\Attacks\\attack_generated_new'\n",
                "BENIGN_FOLDER = r'C:\\Users\\shenal\\Downloads\\reseraach\\Attacks\\Attacks\\benign_generated_org'\n",
                "\n",
                "unseen_results = test_unseen_data(best_rf, ATTACK_FOLDER, BENIGN_FOLDER)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Model Persistence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained model\n",
                "print(\"=\"*80)\n",
                "print(\"MODEL PERSISTENCE\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "MODEL_PATH = 'random_forest_dns_infrastructure_model.pkl'\n",
                "\n",
                "print(f\"\\n‚è≥ Saving model to: {MODEL_PATH}\")\n",
                "joblib.dump(best_rf, MODEL_PATH)\n",
                "\n",
                "# Verify save\n",
                "import os\n",
                "file_size = os.path.getsize(MODEL_PATH) / (1024 * 1024)  # Convert to MB\n",
                "\n",
                "print(f\"‚úì Model saved successfully\")\n",
                "print(f\"  - File: {MODEL_PATH}\")\n",
                "print(f\"  - Size: {file_size:.2f} MB\")\n",
                "\n",
                "# Test loading\n",
                "print(f\"\\n‚è≥ Verifying model can be loaded...\")\n",
                "loaded_model = joblib.load(MODEL_PATH)\n",
                "print(f\"‚úì Model loaded successfully\")\n",
                "print(f\"  - Type: {type(loaded_model).__name__}\")\n",
                "print(f\"  - Features: {loaded_model.n_features_in_}\")\n",
                "print(f\"  - Trees: {loaded_model.n_estimators}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TRAINING SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüìä Model Performance:\")\n",
                "print(f\"  - Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
                "print(f\"  - Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "if unseen_results:\n",
                "    print(f\"  - Unseen Data Accuracy: {unseen_results['accuracy']:.4f} ({unseen_results['accuracy']*100:.2f}%)\")\n",
                "    print(f\"\\nüéØ Goal Achievement:\")\n",
                "    if unseen_results['accuracy'] > 0.72:\n",
                "        print(f\"  ‚úÖ PASSED: Exceeded 72% baseline ({unseen_results['accuracy']*100:.2f}%)\")\n",
                "    else:\n",
                "        print(f\"  ‚ùå FAILED: Did not exceed 72% baseline ({unseen_results['accuracy']*100:.2f}%)\")\n",
                "\n",
                "print(f\"\\nüõ°Ô∏è Overfitting Check:\")\n",
                "gap = abs(train_accuracy - test_accuracy)\n",
                "if gap < 0.05:\n",
                "    print(f\"  ‚úÖ PASSED: Gap = {gap*100:.2f}% (< 5%)\")\n",
                "else:\n",
                "    print(f\"  ‚ö† WARNING: Gap = {gap*100:.2f}% (>= 5%)\")\n",
                "\n",
                "print(f\"\\nüìÅ Saved Model:\")\n",
                "print(f\"  - Path: {MODEL_PATH}\")\n",
                "print(f\"  - Size: {file_size:.2f} MB\")\n",
                "\n",
                "print(f\"\\n‚úì Training pipeline complete!\")\n",
                "print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}